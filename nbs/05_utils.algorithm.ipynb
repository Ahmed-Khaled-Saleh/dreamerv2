{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Title (change me)\n",
    "> Default description (change me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_return(\n",
    "                reward: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                discount: torch.Tensor,\n",
    "                bootstrap: torch.Tensor,\n",
    "                lambda_: float\n",
    "            ):\n",
    "    \"\"\"\n",
    "    Compute the discounted reward for a batch of data.\n",
    "    reward, value, and discount are all shape [horizon - 1, batch, 1] (last element is cut off)\n",
    "    Bootstrap is [batch, 1]\n",
    "    \"\"\"\n",
    "    next_values = torch.cat([value[1:], bootstrap[None]], 0)\n",
    "    target = reward + discount * next_values * (1 - lambda_)\n",
    "    timesteps = list(range(reward.shape[0] - 1, -1, -1))\n",
    "    outputs = []\n",
    "    accumulated_reward = bootstrap\n",
    "    for t in timesteps:\n",
    "        inp = target[t]\n",
    "        discount_factor = discount[t]\n",
    "        accumulated_reward = inp + discount_factor * lambda_ * accumulated_reward\n",
    "        outputs.append(accumulated_reward)\n",
    "    returns = torch.flip(torch.stack(outputs), [0])\n",
    "    return returns\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
