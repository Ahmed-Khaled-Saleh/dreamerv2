{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Title (change me)\n",
    "> Default description (change me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.all import *\n",
    "from fastcore.utils import *\n",
    "import numpy as np\n",
    "import torch \n",
    "from dreamerv2.models.actor import DiscreteActionModel\n",
    "from dreamerv2.models.rssm import RSSM\n",
    "from dreamerv2.models.dense import DenseModel\n",
    "from dreamerv2.models.pixel import ObsDecoder, ObsEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Evaluator(object):\n",
    "    '''\n",
    "    used this only for minigrid envs\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        config,\n",
    "        device,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.action_size = config.action_size\n",
    "\n",
    "    def load_model(self, config, model_path):\n",
    "        saved_dict = torch.load(model_path)\n",
    "        obs_shape = config.obs_shape\n",
    "        action_size = config.action_size\n",
    "        deter_size = config.rssm_info['deter_size']\n",
    "        if config.rssm_type == 'continuous':\n",
    "            stoch_size = config.rssm_info['stoch_size']\n",
    "        elif config.rssm_type == 'discrete':\n",
    "            category_size = config.rssm_info['category_size']\n",
    "            class_size = config.rssm_info['class_size']\n",
    "            stoch_size = category_size*class_size\n",
    "\n",
    "        embedding_size = config.embedding_size\n",
    "        rssm_node_size = config.rssm_node_size\n",
    "        modelstate_size = stoch_size + deter_size \n",
    "\n",
    "        if config.pixel:\n",
    "                self.ObsEncoder = ObsEncoder(obs_shape, embedding_size, config.obs_encoder).to(self.device).eval()\n",
    "                self.ObsDecoder = ObsDecoder(obs_shape, modelstate_size, config.obs_decoder).to(self.device).eval()\n",
    "        else:\n",
    "            self.ObsEncoder = DenseModel((embedding_size,), int(np.prod(obs_shape)), config.obs_encoder).to(self.device).eval()\n",
    "            self.ObsDecoder = DenseModel(obs_shape, modelstate_size, config.obs_decoder).to(self.device).eval()\n",
    "\n",
    "        self.ActionModel = DiscreteActionModel(action_size, deter_size, stoch_size, embedding_size, config.actor, config.expl).to(self.device).eval()\n",
    "        self.RSSM = RSSM(action_size, rssm_node_size, embedding_size, self.device, config.rssm_type, config.rssm_info).to(self.device).eval()\n",
    "\n",
    "        self.RSSM.load_state_dict(saved_dict[\"RSSM\"])\n",
    "        self.ObsEncoder.load_state_dict(saved_dict[\"ObsEncoder\"])\n",
    "        self.ObsDecoder.load_state_dict(saved_dict[\"ObsDecoder\"])\n",
    "        self.ActionModel.load_state_dict(saved_dict[\"ActionModel\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def eval_saved_agent(self: Evaluator, env, model_path):\n",
    "    self.load_model(self.config, model_path)\n",
    "    eval_episode = self.config.eval_episode\n",
    "    eval_scores = []    \n",
    "    for e in range(eval_episode):\n",
    "        obs, score = env.reset(), 0\n",
    "        done = False\n",
    "        prev_rssmstate = self.RSSM._init_rssm_state(1)\n",
    "        prev_action = torch.zeros(1, self.action_size).to(self.device)\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                embed = self.ObsEncoder(torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(self.device))    \n",
    "                _, posterior_rssm_state = self.RSSM.rssm_observe(embed, prev_action, not done, prev_rssmstate)\n",
    "                model_state = self.RSSM.get_model_state(posterior_rssm_state)\n",
    "                action, _ = self.ActionModel(model_state)\n",
    "                prev_rssmstate = posterior_rssm_state\n",
    "                prev_action = action\n",
    "            next_obs, rew, done, _ = env.step(action.squeeze(0).cpu().numpy())\n",
    "            if self.config.eval_render:\n",
    "                env.render()\n",
    "            score += rew\n",
    "            obs = next_obs\n",
    "        eval_scores.append(score)\n",
    "    print('average evaluation score for model at ' + model_path + ' = ' +str(np.mean(eval_scores)))\n",
    "    env.close()\n",
    "    return np.mean(eval_scores)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
