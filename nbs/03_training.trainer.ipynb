{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Title (change me)\n",
    "> Default description (change me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import os \n",
    "\n",
    "from dreamerv2.utils.module import get_parameters, FreezeParameters\n",
    "from dreamerv2.utils.algorithm import compute_return\n",
    "\n",
    "from dreamerv2.models.actor import DiscreteActionModel\n",
    "from dreamerv2.models.dense import DenseModel\n",
    "from dreamerv2.models.rssm import RSSM\n",
    "from dreamerv2.models.pixel import ObsDecoder, ObsEncoder\n",
    "from dreamerv2.utils.buffer import TransitionBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        config,\n",
    "        device,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.action_size = config.action_size\n",
    "        self.pixel = config.pixel\n",
    "        self.kl_info = config.kl\n",
    "        self.seq_len = config.seq_len\n",
    "        self.batch_size = config.batch_size\n",
    "        self.collect_intervals = config.collect_intervals\n",
    "        self.seed_steps = config.seed_steps\n",
    "        self.discount = config.discount_\n",
    "        self.lambda_ = config.lambda_\n",
    "        self.horizon = config.horizon\n",
    "        self.loss_scale = config.loss_scale\n",
    "        self.actor_entropy_scale = config.actor_entropy_scale\n",
    "        self.grad_clip_norm = config.grad_clip\n",
    "\n",
    "        self._model_initialize(config)\n",
    "        self._optim_initialize(config)\n",
    "\n",
    "    def collect_seed_episodes(self, env):\n",
    "        s, done  = env.reset(), False \n",
    "        for i in range(self.seed_steps):\n",
    "            a = env.action_space.sample()\n",
    "            ns, r, done, _ = env.step(a)\n",
    "            if done:\n",
    "                self.buffer.add(s,a,r,done)\n",
    "                s, done  = env.reset(), False \n",
    "            else:\n",
    "                self.buffer.add(s,a,r,done)\n",
    "                s = ns    \n",
    "\n",
    "    def train_batch(self, train_metrics):\n",
    "        \"\"\" \n",
    "        trains the world model and imagination actor and critic for collect_interval times using sequence-batch data from buffer\n",
    "        \"\"\"\n",
    "        actor_l = []\n",
    "        value_l = []\n",
    "        obs_l = []\n",
    "        model_l = []\n",
    "        reward_l = []\n",
    "        prior_ent_l = []\n",
    "        post_ent_l = []\n",
    "        kl_l = []\n",
    "        pcont_l = []\n",
    "        mean_targ = []\n",
    "        min_targ = []\n",
    "        max_targ = []\n",
    "        std_targ = []\n",
    "\n",
    "        for i in range(self.collect_intervals):\n",
    "            obs, actions, rewards, terms = self.buffer.sample()\n",
    "            obs = torch.tensor(obs, dtype=torch.float32).to(self.device)                         #t, t+seq_len \n",
    "            actions = torch.tensor(actions, dtype=torch.float32).to(self.device)                 #t-1, t+seq_len-1\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device).unsqueeze(-1)   #t-1 to t+seq_len-1\n",
    "            nonterms = torch.tensor(1-terms, dtype=torch.float32).to(self.device).unsqueeze(-1)  #t-1 to t+seq_len-1\n",
    "\n",
    "            model_loss, kl_loss, obs_loss, reward_loss, pcont_loss, prior_dist, post_dist, posterior = self.representation_loss(obs, actions, rewards, nonterms)\n",
    "            \n",
    "            self.model_optimizer.zero_grad()\n",
    "            model_loss.backward()\n",
    "            grad_norm_model = torch.nn.utils.clip_grad_norm_(get_parameters(self.world_list), self.grad_clip_norm)\n",
    "            self.model_optimizer.step()\n",
    "\n",
    "            actor_loss, value_loss, target_info = self.actorcritc_loss(posterior)\n",
    "\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.value_optimizer.zero_grad()\n",
    "\n",
    "            actor_loss.backward()\n",
    "            value_loss.backward()\n",
    "\n",
    "            grad_norm_actor = torch.nn.utils.clip_grad_norm_(get_parameters(self.actor_list), self.grad_clip_norm)\n",
    "            grad_norm_value = torch.nn.utils.clip_grad_norm_(get_parameters(self.value_list), self.grad_clip_norm)\n",
    "\n",
    "            self.actor_optimizer.step()\n",
    "            self.value_optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prior_ent = torch.mean(prior_dist.entropy())\n",
    "                post_ent = torch.mean(post_dist.entropy())\n",
    "\n",
    "            prior_ent_l.append(prior_ent.item())\n",
    "            post_ent_l.append(post_ent.item())\n",
    "            actor_l.append(actor_loss.item())\n",
    "            value_l.append(value_loss.item())\n",
    "            obs_l.append(obs_loss.item())\n",
    "            model_l.append(model_loss.item())\n",
    "            reward_l.append(reward_loss.item())\n",
    "            kl_l.append(kl_loss.item())\n",
    "            pcont_l.append(pcont_loss.item())\n",
    "            mean_targ.append(target_info['mean_targ'])\n",
    "            min_targ.append(target_info['min_targ'])\n",
    "            max_targ.append(target_info['max_targ'])\n",
    "            std_targ.append(target_info['std_targ'])\n",
    "\n",
    "        train_metrics['model_loss'] = np.mean(model_l)\n",
    "        train_metrics['kl_loss']=np.mean(kl_l)\n",
    "        train_metrics['reward_loss']=np.mean(reward_l)\n",
    "        train_metrics['obs_loss']=np.mean(obs_l)\n",
    "        train_metrics['value_loss']=np.mean(value_l)\n",
    "        train_metrics['actor_loss']=np.mean(actor_l)\n",
    "        train_metrics['prior_entropy']=np.mean(prior_ent_l)\n",
    "        train_metrics['posterior_entropy']=np.mean(post_ent_l)\n",
    "        train_metrics['pcont_loss']=np.mean(pcont_l)\n",
    "        train_metrics['mean_targ']=np.mean(mean_targ)\n",
    "        train_metrics['min_targ']=np.mean(min_targ)\n",
    "        train_metrics['max_targ']=np.mean(max_targ)\n",
    "        train_metrics['std_targ']=np.mean(std_targ)\n",
    "\n",
    "        return train_metrics\n",
    "\n",
    "    def actorcritc_loss(self, posterior):\n",
    "        with torch.no_grad():\n",
    "            batched_posterior = self.RSSM.rssm_detach(self.RSSM.rssm_seq_to_batch(posterior, self.batch_size, self.seq_len-1))\n",
    "        \n",
    "        with FreezeParameters(self.world_list):\n",
    "            imag_rssm_states, imag_log_prob, policy_entropy = self.RSSM.rollout_imagination(self.horizon, self.ActionModel, batched_posterior)\n",
    "        \n",
    "        imag_modelstates = self.RSSM.get_model_state(imag_rssm_states)\n",
    "        with FreezeParameters(self.world_list+self.value_list+[self.TargetValueModel]+[self.DiscountModel]):\n",
    "            imag_reward_dist = self.RewardDecoder(imag_modelstates)\n",
    "            imag_reward = imag_reward_dist.mean\n",
    "            imag_value_dist = self.TargetValueModel(imag_modelstates)\n",
    "            imag_value = imag_value_dist.mean\n",
    "            discount_dist = self.DiscountModel(imag_modelstates)\n",
    "            discount_arr = self.discount*torch.round(discount_dist.base_dist.probs)              #mean = prob(disc==1)\n",
    "\n",
    "        actor_loss, discount, lambda_returns = self._actor_loss(imag_reward, imag_value, discount_arr, imag_log_prob, policy_entropy)\n",
    "        value_loss = self._value_loss(imag_modelstates, discount, lambda_returns)     \n",
    "\n",
    "        mean_target = torch.mean(lambda_returns, dim=1)\n",
    "        max_targ = torch.max(mean_target).item()\n",
    "        min_targ = torch.min(mean_target).item() \n",
    "        std_targ = torch.std(mean_target).item()\n",
    "        mean_targ = torch.mean(mean_target).item()\n",
    "        target_info = {\n",
    "            'min_targ':min_targ,\n",
    "            'max_targ':max_targ,\n",
    "            'std_targ':std_targ,\n",
    "            'mean_targ':mean_targ,\n",
    "        }\n",
    "\n",
    "        return actor_loss, value_loss, target_info\n",
    "\n",
    "    def representation_loss(self, obs, actions, rewards, nonterms):\n",
    "\n",
    "        embed = self.ObsEncoder(obs)                                         #t to t+seq_len   \n",
    "        prev_rssm_state = self.RSSM._init_rssm_state(self.batch_size)   \n",
    "        prior, posterior = self.RSSM.rollout_observation(self.seq_len, embed, actions, nonterms, prev_rssm_state)\n",
    "        post_modelstate = self.RSSM.get_model_state(posterior)               #t to t+seq_len   \n",
    "        obs_dist = self.ObsDecoder(post_modelstate[:-1])                     #t to t+seq_len-1  \n",
    "        reward_dist = self.RewardDecoder(post_modelstate[:-1])               #t to t+seq_len-1  \n",
    "        pcont_dist = self.DiscountModel(post_modelstate[:-1])                #t to t+seq_len-1   \n",
    "        \n",
    "        obs_loss = self._obs_loss(obs_dist, obs[:-1])\n",
    "        reward_loss = self._reward_loss(reward_dist, rewards[1:])\n",
    "        pcont_loss = self._pcont_loss(pcont_dist, nonterms[1:])\n",
    "        prior_dist, post_dist, div = self._kl_loss(prior, posterior)\n",
    "\n",
    "        model_loss = self.loss_scale['kl'] * div + reward_loss + obs_loss + self.loss_scale['discount']*pcont_loss\n",
    "        return model_loss, div, obs_loss, reward_loss, pcont_loss, prior_dist, post_dist, posterior\n",
    "\n",
    "    def _actor_loss(self, imag_reward, imag_value, discount_arr, imag_log_prob, policy_entropy):\n",
    "\n",
    "        lambda_returns = compute_return(imag_reward[:-1], imag_value[:-1], discount_arr[:-1], bootstrap=imag_value[-1], lambda_=self.lambda_)\n",
    "        \n",
    "        if self.config.actor_grad == 'reinforce':\n",
    "            advantage = (lambda_returns-imag_value[:-1]).detach()\n",
    "            objective = imag_log_prob[1:].unsqueeze(-1) * advantage\n",
    "\n",
    "        elif self.config.actor_grad == 'dynamics':\n",
    "            objective = lambda_returns\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        discount_arr = torch.cat([torch.ones_like(discount_arr[:1]), discount_arr[1:]])\n",
    "        discount = torch.cumprod(discount_arr[:-1], 0)\n",
    "        policy_entropy = policy_entropy[1:].unsqueeze(-1)\n",
    "        actor_loss = -torch.sum(torch.mean(discount * (objective + self.actor_entropy_scale * policy_entropy), dim=1)) \n",
    "        return actor_loss, discount, lambda_returns\n",
    "\n",
    "    def _value_loss(self, imag_modelstates, discount, lambda_returns):\n",
    "        with torch.no_grad():\n",
    "            value_modelstates = imag_modelstates[:-1].detach()\n",
    "            value_discount = discount.detach()\n",
    "            value_target = lambda_returns.detach()\n",
    "\n",
    "        value_dist = self.ValueModel(value_modelstates) \n",
    "        value_loss = -torch.mean(value_discount*value_dist.log_prob(value_target).unsqueeze(-1))\n",
    "        return value_loss\n",
    "            \n",
    "    def _obs_loss(self, obs_dist, obs):\n",
    "        obs_loss = -torch.mean(obs_dist.log_prob(obs))\n",
    "        return obs_loss\n",
    "    \n",
    "    def _kl_loss(self, prior, posterior):\n",
    "        prior_dist = self.RSSM.get_dist(prior)\n",
    "        post_dist = self.RSSM.get_dist(posterior)\n",
    "        if self.kl_info['use_kl_balance']:\n",
    "            alpha = self.kl_info['kl_balance_scale']\n",
    "            kl_lhs = torch.mean(torch.distributions.kl.kl_divergence(self.RSSM.get_dist(self.RSSM.rssm_detach(posterior)), prior_dist))\n",
    "            kl_rhs = torch.mean(torch.distributions.kl.kl_divergence(post_dist, self.RSSM.get_dist(self.RSSM.rssm_detach(prior))))\n",
    "            if self.kl_info['use_free_nats']:\n",
    "                free_nats = self.kl_info['free_nats']\n",
    "                kl_lhs = torch.max(kl_lhs,kl_lhs.new_full(kl_lhs.size(), free_nats))\n",
    "                kl_rhs = torch.max(kl_rhs,kl_rhs.new_full(kl_rhs.size(), free_nats))\n",
    "            kl_loss = alpha*kl_lhs + (1-alpha)*kl_rhs\n",
    "\n",
    "        else: \n",
    "            kl_loss = torch.mean(torch.distributions.kl.kl_divergence(post_dist, prior_dist))\n",
    "            if self.kl_info['use_free_nats']:\n",
    "                free_nats = self.kl_info['free_nats']\n",
    "                kl_loss = torch.max(kl_loss, kl_loss.new_full(kl_loss.size(), free_nats))\n",
    "        return prior_dist, post_dist, kl_loss\n",
    "    \n",
    "    def _reward_loss(self, reward_dist, rewards):\n",
    "        reward_loss = -torch.mean(reward_dist.log_prob(rewards))\n",
    "        return reward_loss\n",
    "    \n",
    "    def _pcont_loss(self, pcont_dist, nonterms):\n",
    "        pcont_target = nonterms.float()\n",
    "        pcont_loss = -torch.mean(pcont_dist.log_prob(pcont_target))\n",
    "        return pcont_loss\n",
    "\n",
    "    def update_target(self):\n",
    "        mix = self.config.slow_target_fraction if self.config.use_slow_target else 1\n",
    "        for param, target_param in zip(self.ValueModel.parameters(), self.TargetValueModel.parameters()):\n",
    "            target_param.data.copy_(mix * param.data + (1 - mix) * target_param.data)\n",
    "\n",
    "    def save_model(self, iter):\n",
    "        save_dict = self.get_save_dict()\n",
    "        model_dir = self.config.model_dir\n",
    "        save_path = os.path.join(model_dir, 'models_%d.pth' % iter)\n",
    "        torch.save(save_dict, save_path)\n",
    "\n",
    "    def get_save_dict(self):\n",
    "        return {\n",
    "            \"RSSM\": self.RSSM.state_dict(),\n",
    "            \"ObsEncoder\": self.ObsEncoder.state_dict(),\n",
    "            \"ObsDecoder\": self.ObsDecoder.state_dict(),\n",
    "            \"RewardDecoder\": self.RewardDecoder.state_dict(),\n",
    "            \"ActionModel\": self.ActionModel.state_dict(),\n",
    "            \"ValueModel\": self.ValueModel.state_dict(),\n",
    "            \"DiscountModel\": self.DiscountModel.state_dict(),\n",
    "        }\n",
    "    \n",
    "    def load_save_dict(self, saved_dict):\n",
    "        self.RSSM.load_state_dict(saved_dict[\"RSSM\"])\n",
    "        self.ObsEncoder.load_state_dict(saved_dict[\"ObsEncoder\"])\n",
    "        self.ObsDecoder.load_state_dict(saved_dict[\"ObsDecoder\"])\n",
    "        self.RewardDecoder.load_state_dict(saved_dict[\"RewardDecoder\"])\n",
    "        self.ActionModel.load_state_dict(saved_dict[\"ActionModel\"])\n",
    "        self.ValueModel.load_state_dict(saved_dict[\"ValueModel\"])\n",
    "        self.DiscountModel.load_state_dict(saved_dict['DiscountModel'])\n",
    "            \n",
    "    def _model_initialize(self, config):\n",
    "        obs_shape = config.obs_shape\n",
    "        action_size = config.action_size\n",
    "        deter_size = config.rssm_info['deter_size']\n",
    "        if config.rssm_type == 'continuous':\n",
    "            stoch_size = config.rssm_info['stoch_size']\n",
    "        elif config.rssm_type == 'discrete':\n",
    "            category_size = config.rssm_info['category_size']\n",
    "            class_size = config.rssm_info['class_size']\n",
    "            stoch_size = category_size*class_size\n",
    "\n",
    "        embedding_size = config.embedding_size\n",
    "        rssm_node_size = config.rssm_node_size\n",
    "        modelstate_size = stoch_size + deter_size \n",
    "    \n",
    "        self.buffer = TransitionBuffer(config.capacity, obs_shape, action_size, config.seq_len, config.batch_size, config.obs_dtype, config.action_dtype)\n",
    "        self.RSSM = RSSM(action_size, rssm_node_size, embedding_size, self.device, config.rssm_type, config.rssm_info).to(self.device)\n",
    "        self.ActionModel = DiscreteActionModel(action_size, deter_size, stoch_size, embedding_size, config.actor, config.expl).to(self.device)\n",
    "        self.RewardDecoder = DenseModel((1,), modelstate_size, config.reward).to(self.device)\n",
    "        self.ValueModel = DenseModel((1,), modelstate_size, config.critic).to(self.device)\n",
    "        self.TargetValueModel = DenseModel((1,), modelstate_size, config.critic).to(self.device)\n",
    "        self.TargetValueModel.load_state_dict(self.ValueModel.state_dict())\n",
    "        \n",
    "        if config.discount['use']:\n",
    "            self.DiscountModel = DenseModel((1,), modelstate_size, config.discount).to(self.device)\n",
    "        if config.pixel:\n",
    "            self.ObsEncoder = ObsEncoder(obs_shape, embedding_size, config.obs_encoder).to(self.device)\n",
    "            self.ObsDecoder = ObsDecoder(obs_shape, modelstate_size, config.obs_decoder).to(self.device)\n",
    "        else:\n",
    "            self.ObsEncoder = DenseModel((embedding_size,), int(np.prod(obs_shape)), config.obs_encoder).to(self.device)\n",
    "            self.ObsDecoder = DenseModel(obs_shape, modelstate_size, config.obs_decoder).to(self.device)\n",
    "\n",
    "    def _optim_initialize(self, config):\n",
    "        model_lr = config.lr['model']\n",
    "        actor_lr = config.lr['actor']\n",
    "        value_lr = config.lr['critic']\n",
    "        self.world_list = [self.ObsEncoder, self.RSSM, self.RewardDecoder, self.ObsDecoder, self.DiscountModel]\n",
    "        self.actor_list = [self.ActionModel]\n",
    "        self.value_list = [self.ValueModel]\n",
    "        self.actorcritic_list = [self.ActionModel, self.ValueModel]\n",
    "        self.model_optimizer = optim.Adam(get_parameters(self.world_list), model_lr)\n",
    "        self.actor_optimizer = optim.Adam(get_parameters(self.actor_list), actor_lr)\n",
    "        self.value_optimizer = optim.Adam(get_parameters(self.value_list), value_lr)\n",
    "\n",
    "    def _print_summary(self):\n",
    "        print('\\n Obs encoder: \\n', self.ObsEncoder)\n",
    "        print('\\n RSSM model: \\n', self.RSSM)\n",
    "        print('\\n Reward decoder: \\n', self.RewardDecoder)\n",
    "        print('\\n Obs decoder: \\n', self.ObsDecoder)\n",
    "        if self.config.discount['use']:\n",
    "            print('\\n Discount decoder: \\n', self.DiscountModel)\n",
    "        print('\\n Actor: \\n', self.ActionModel)\n",
    "        print('\\n Critic: \\n', self.ValueModel)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
