{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Title (change me)\n",
    "> Default description (change me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np \n",
    "import random \n",
    "from collections import namedtuple, deque\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TransitionBuffer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity,\n",
    "        obs_shape: Tuple[int],\n",
    "        action_size: int,\n",
    "        seq_len: int, \n",
    "        batch_size: int,\n",
    "        obs_type=np.float32,\n",
    "        action_type=np.float32,\n",
    "    ):\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_size = action_size\n",
    "        self.obs_type = obs_type\n",
    "        self.action_type = action_type\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.idx = 0\n",
    "        self.full = False\n",
    "        self.observation = np.empty((capacity, *obs_shape), dtype=obs_type) \n",
    "        self.action = np.empty((capacity, action_size), dtype=np.float32)\n",
    "        self.reward = np.empty((capacity,), dtype=np.float32) \n",
    "        self.terminal = np.empty((capacity,), dtype=bool)\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: float,\n",
    "        done: bool,\n",
    "    ):\n",
    "        self.observation[self.idx] = obs\n",
    "        self.action[self.idx] = action \n",
    "        self.reward[self.idx] = reward\n",
    "        self.terminal[self.idx] = done\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "        self.full = self.full or self.idx == 0\n",
    "\n",
    "    def _sample_idx(self, L):\n",
    "        valid_idx = False\n",
    "        while not valid_idx:\n",
    "            idx = np.random.randint(0, self.capacity if self.full else self.idx - L)\n",
    "            idxs = np.arange(idx, idx + L) % self.capacity\n",
    "            valid_idx = not self.idx in idxs[1:] \n",
    "        return idxs\n",
    "\n",
    "    def _retrieve_batch(self, idxs, n, l):\n",
    "        vec_idxs = idxs.transpose().reshape(-1)\n",
    "        observation = self.observation[vec_idxs]\n",
    "        return observation.reshape(l, n, *self.obs_shape), self.action[vec_idxs].reshape(l, n, -1), self.reward[vec_idxs].reshape(l, n), self.terminal[vec_idxs].reshape(l, n)\n",
    "\n",
    "    def sample(self):\n",
    "        n = self.batch_size\n",
    "        l = self.seq_len+1\n",
    "        obs,act,rew,term = self._retrieve_batch(np.asarray([self._sample_idx(l) for _ in range(n)]), n, l)\n",
    "        obs,act,rew,term = self._shift_sequences(obs,act,rew,term)\n",
    "        return obs,act,rew,term\n",
    "    \n",
    "    def _shift_sequences(self, obs, actions, rewards, terminals):\n",
    "        obs = obs[1:]\n",
    "        actions = actions[:-1]\n",
    "        rewards = rewards[:-1]\n",
    "        terminals = terminals[:-1]\n",
    "        return obs, actions, rewards, terminals\n",
    "\n",
    "#Following objects are not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "Episode = namedtuple('Episode', ['observation', 'action', 'reward', 'terminal', 'length'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EpisodicBuffer():\n",
    "    \"\"\"\n",
    "    :params total_episodes: maximum no of episodes capacity  \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_episodes,\n",
    "        obs_shape: Tuple[int],\n",
    "        action_size: int,\n",
    "        seq_len: int, \n",
    "        batch_size: int,\n",
    "        obs_type=np.float32,\n",
    "        action_type=np.float32,\n",
    "    ):\n",
    "        self.total_episodes = total_episodes\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_size = action_size\n",
    "        self.obs_type = obs_type\n",
    "        self.action_type = action_type\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque([], maxlen=total_episodes)\n",
    "        self.lengths = deque([], maxlen=total_episodes)\n",
    "        self._full = False\n",
    "        self._episode_cnt = 0\n",
    "        self._init_episode()\n",
    "    \n",
    "    def add(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: float,\n",
    "        done: bool,\n",
    "        last_obs: Optional[np.ndarray] = None\n",
    "    ):\n",
    "        self.observation.append(obs)\n",
    "        self.action.append(action)\n",
    "        self.reward.append(reward)\n",
    "        self.terminal.append(done)\n",
    "        \n",
    "        if done:\n",
    "            assert last_obs is not None\n",
    "            self.observation.append(last_obs)\n",
    "            self.add_episode()\n",
    "    \n",
    "    def sample(self):\n",
    "        seq_len = self.seq_len\n",
    "        batch_size = self.batch_size\n",
    "        episode_list = random.choices(self.buffer, k=batch_size)\n",
    "        obs_batch = np.empty([seq_len, batch_size, *self.obs_shape], dtype=self.obs_type)\n",
    "        act_batch = np.empty([seq_len, batch_size, self.action_size], dtype=self.action_type)\n",
    "        rew_batch = np.empty([seq_len, batch_size], dtype=np.float32)\n",
    "        term_batch = np.empty([seq_len, batch_size], dtype=bool)\n",
    "        for ind, episode in enumerate(episode_list):\n",
    "            obs_batch[:,ind], act_batch[:,ind], rew_batch[:,ind], term_batch[:,ind] = self._sample_seq(episode, seq_len)\n",
    "        return obs_batch, act_batch, rew_batch , term_batch\n",
    "    \n",
    "    def _sample_seq(self, episode, seq_len):\n",
    "        assert episode.length>=seq_len\n",
    "        s = min(np.random.choice(episode.length), episode.length-seq_len)\n",
    "        return (\n",
    "                episode.observation[s:s+seq_len], \n",
    "                episode.action[s:s+seq_len], \n",
    "                episode.reward[s:s+seq_len], \n",
    "                episode.terminal[s:s+seq_len]\n",
    "            )\n",
    "    \n",
    "    def add_episode(self):\n",
    "        assert self.terminal[-1] == True\n",
    "        e = Episode(*self._episode_to_array(), len(self.terminal))\n",
    "        self.buffer.append(e)\n",
    "        self._episode_cnt += 1\n",
    "        if self._episode_cnt == self.total_episodes:\n",
    "            self.full = True \n",
    "        self._init_episode()\n",
    "    \n",
    "    def _init_episode(self):\n",
    "        self.observation = [] \n",
    "        self.action = [np.zeros(self.action_size, dtype=self.action_type)] \n",
    "        self.reward = [0.0]\n",
    "        self.terminal = [False]\n",
    "    \n",
    "    def _episode_to_array(self):\n",
    "        o = np.stack(self.observation, axis=0)\n",
    "        a = np.stack(self.action, axis=0)\n",
    "        r = np.stack(self.reward, axis=0)\n",
    "        nt = np.stack(self.terminal, axis=0)\n",
    "        return o,a,r,nt\n",
    "\n",
    "    @property\n",
    "    def episode_count(self):\n",
    "        return self._episode_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FluidEpisodicBuffer():\n",
    "    \"\"\"\n",
    "    :params total_episodes: \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_episodes: int,\n",
    "        obs_shape: Tuple[int],\n",
    "        action_size: int,\n",
    "        seq_len: int, \n",
    "        batch_size: int,\n",
    "        minimum_episode_len: Optional[int] = 1,\n",
    "        obs_type: Optional[np.dtype] = np.uint8,\n",
    "        action_type: Optional[np.dtype] = np.float32,\n",
    "        incr_len: Optional[int] = 5,\n",
    "    ):\n",
    "        self.total_episodes = total_episodes\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_size = action_size\n",
    "        self.obs_type = obs_type\n",
    "        self.action_type = action_type\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.incr_len = incr_len\n",
    "\n",
    "        self.buffer = deque([], maxlen=total_episodes)\n",
    "        self.lengths = deque([], maxlen=total_episodes)\n",
    "        \n",
    "        self._minimum_episode_len = minimum_episode_len\n",
    "        self.opt_frac = 0.1*total_episodes\n",
    "        self.opt_seq_len = minimum_episode_len\n",
    "        self._init_episode()\n",
    "    \n",
    "    def add(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: float,\n",
    "        done: bool,\n",
    "        last_obs: Optional[np.ndarray] = None\n",
    "    ):\n",
    "\n",
    "        self.observation.append(obs)\n",
    "        self.action.append(action)\n",
    "        self.reward.append(reward)\n",
    "        self.terminal.append(done)\n",
    "        \n",
    "        if done:\n",
    "            assert last_obs is not None\n",
    "            self.observation.append(last_obs)\n",
    "            self.add_episode()\n",
    "    \n",
    "    def sample(self):\n",
    "        seq_len = self.opt_seq_len\n",
    "        batch_size = self.batch_size\n",
    "        obs_batch = np.empty([seq_len, batch_size, *self.obs_shape], dtype=self.obs_type)\n",
    "        act_batch = np.empty([seq_len, batch_size, self.action_size], dtype=self.action_type)\n",
    "        rew_batch = np.empty([seq_len, batch_size], dtype=np.float32)\n",
    "        term_batch = np.empty([seq_len, batch_size], dtype=bool)\n",
    "        episode_idx = np.random.choice(np.where(np.array(self.lengths)>=seq_len)[0], size=batch_size)\n",
    "        for i,idx in enumerate(episode_idx):\n",
    "            obs_batch[:,i], act_batch[:,i], rew_batch[:,i], term_batch[:,i] = self._sample_seq(self.buffer[idx], self.lengths[idx], seq_len)\n",
    "        return obs_batch, act_batch, rew_batch , term_batch\n",
    "    \n",
    "    def _sample_seq(self, episode, episode_len, seq_len):\n",
    "        s = min(episode_len, episode_len-seq_len)\n",
    "        return (\n",
    "                episode.observation[s:s+seq_len], \n",
    "                episode.action[s:s+seq_len], \n",
    "                episode.reward[s:s+seq_len], \n",
    "                episode.terminal[s:s+seq_len]\n",
    "            )\n",
    "    \n",
    "    def add_episode(self):\n",
    "        assert self.terminal[-1] == True\n",
    "        e = Episode(*self._episode_to_array())\n",
    "        if len(self.terminal)>=self.opt_seq_len:  \n",
    "            self.buffer.append(e)\n",
    "            self.lengths.append(len(self.terminal))\n",
    "        self._init_episode()\n",
    "        self._set_opt_len()\n",
    "    \n",
    "    def _set_opt_len(self):\n",
    "        temp_len = np.array(self.lengths)\n",
    "        if np.sum(temp_len>self.opt_seq_len+self.incr_len)>self.opt_frac:\n",
    "            self.opt_seq_len = min(self.opt_seq_len+self.incr_len, self.seq_len)\n",
    "    \n",
    "    def _init_episode(self):\n",
    "        self.observation = [] \n",
    "        self.action = [np.zeros(self.action_size, dtype=self.action_type)] \n",
    "        self.reward = [0.0]\n",
    "        self.terminal = [False]\n",
    "    \n",
    "    def _episode_to_array(self):\n",
    "        o = np.stack(self.observation, axis=0)\n",
    "        a = np.stack(self.action, axis=0)\n",
    "        r = np.stack(self.reward, axis=0)\n",
    "        nt = np.stack(self.terminal, axis=0)\n",
    "        return o,a,r,nt"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
