{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4bdaba",
   "metadata": {},
   "source": [
    "# RSSM\n",
    "\n",
    "> Recurrent SSM for Prior, Posterior and Transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.rssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as td\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dreamerv2.utils.rssm import RSSMUtils, RSSMContState, RSSMDiscState\n",
    "from fastcore.utils import *\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9211e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RSSM(nn.Module, RSSMUtils):\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_size,\n",
    "        rssm_node_size,\n",
    "        embedding_size,\n",
    "        device,\n",
    "        rssm_type,\n",
    "        info,\n",
    "        act_fn=nn.ELU,  \n",
    "    ):\n",
    "        nn.Module.__init__(self)\n",
    "        RSSMUtils.__init__(self, rssm_type=rssm_type, info=info)\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        self.node_size = rssm_node_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.act_fn = act_fn\n",
    "        self.rnn = nn.GRUCell(self.deter_size, self.deter_size)\n",
    "        self.fc_embed_state_action = self._build_embed_state_action()\n",
    "        self.fc_prior = self._build_temporal_prior()\n",
    "        self.fc_posterior = self._build_temporal_posterior()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4469ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _build_embed_state_action(self: RSSM) -> nn.Sequential:\n",
    "    \"\"\"\n",
    "    model is supposed to take in previous stochastic state and previous action \n",
    "    and embed it to deter size for rnn input\n",
    "    \"\"\"\n",
    "    fc_embed_state_action = [nn.Linear(self.stoch_size + self.action_size, self.deter_size)]\n",
    "    fc_embed_state_action += [self.act_fn()]\n",
    "    return nn.Sequential(*fc_embed_state_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _build_temporal_prior(self: RSSM) -> nn.Sequential:\n",
    "    \"\"\"\n",
    "    model is supposed to take in latest deterministic state \n",
    "    and output prior over stochastic state\n",
    "    \"\"\"\n",
    "    temporal_prior = [nn.Linear(self.deter_size, self.node_size)]\n",
    "    temporal_prior += [self.act_fn()]\n",
    "    if self.rssm_type == 'discrete':\n",
    "        temporal_prior += [nn.Linear(self.node_size, self.stoch_size)]\n",
    "    elif self.rssm_type == 'continuous':\n",
    "            temporal_prior += [nn.Linear(self.node_size, 2 * self.stoch_size)]\n",
    "    return nn.Sequential(*temporal_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c73388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _build_temporal_posterior(self: RSSM) -> nn.Sequential:\n",
    "    \"\"\"\n",
    "    model is supposed to take in latest embedded observation and deterministic state \n",
    "    and output posterior over stochastic states\n",
    "    \"\"\"\n",
    "    temporal_posterior = [nn.Linear(self.deter_size + self.embedding_size, self.node_size)]\n",
    "    temporal_posterior += [self.act_fn()]\n",
    "    if self.rssm_type == 'discrete':\n",
    "        temporal_posterior += [nn.Linear(self.node_size, self.stoch_size)]\n",
    "    elif self.rssm_type == 'continuous':\n",
    "        temporal_posterior += [nn.Linear(self.node_size, 2 * self.stoch_size)]\n",
    "    return nn.Sequential(*temporal_posterior)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def rssm_imagine(self: RSSM, prev_action, prev_rssm_state, nonterms=True):\n",
    "    state_action_embed = self.fc_embed_state_action(torch.cat([prev_rssm_state.stoch*nonterms, prev_action],dim=-1))\n",
    "    deter_state = self.rnn(state_action_embed, prev_rssm_state.deter*nonterms)\n",
    "    if self.rssm_type == 'discrete':\n",
    "        prior_logit = self.fc_prior(deter_state)\n",
    "        stats = {'logit':prior_logit}\n",
    "        prior_stoch_state = self.get_stoch_state(stats)\n",
    "        prior_rssm_state = RSSMDiscState(prior_logit, prior_stoch_state, deter_state)\n",
    "\n",
    "    elif self.rssm_type == 'continuous':\n",
    "        prior_mean, prior_std = torch.chunk(self.fc_prior(deter_state), 2, dim=-1)\n",
    "        stats = {'mean':prior_mean, 'std':prior_std}\n",
    "        prior_stoch_state, std = self.get_stoch_state(stats)\n",
    "        prior_rssm_state = RSSMContState(prior_mean, std, prior_stoch_state, deter_state)\n",
    "    return prior_rssm_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5caacd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def rollout_imagination(self: RSSM, horizon:int, actor:nn.Module, prev_rssm_state):\n",
    "        rssm_state = prev_rssm_state\n",
    "        next_rssm_states = []\n",
    "        action_entropy = []\n",
    "        imag_log_probs = []\n",
    "        for t in range(horizon):\n",
    "            action, action_dist = actor((self.get_model_state(rssm_state)).detach())\n",
    "            rssm_state = self.rssm_imagine(action, rssm_state)\n",
    "            next_rssm_states.append(rssm_state)\n",
    "            action_entropy.append(action_dist.entropy())\n",
    "            imag_log_probs.append(action_dist.log_prob(torch.round(action.detach())))\n",
    "\n",
    "        next_rssm_states = self.rssm_stack_states(next_rssm_states, dim=0)\n",
    "        action_entropy = torch.stack(action_entropy, dim=0)\n",
    "        imag_log_probs = torch.stack(imag_log_probs, dim=0)\n",
    "        return next_rssm_states, imag_log_probs, action_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e051259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def rssm_observe(self: RSSM, obs_embed, prev_action, prev_nonterm, prev_rssm_state):\n",
    "    prior_rssm_state = self.rssm_imagine(prev_action, prev_rssm_state, prev_nonterm)\n",
    "    deter_state = prior_rssm_state.deter\n",
    "    x = torch.cat([deter_state, obs_embed], dim=-1)\n",
    "    if self.rssm_type == 'discrete':\n",
    "        posterior_logit = self.fc_posterior(x)\n",
    "        stats = {'logit':posterior_logit}\n",
    "        posterior_stoch_state = self.get_stoch_state(stats)\n",
    "        posterior_rssm_state = RSSMDiscState(posterior_logit, posterior_stoch_state, deter_state)\n",
    "    \n",
    "    elif self.rssm_type == 'continuous':\n",
    "        posterior_mean, posterior_std = torch.chunk(self.fc_posterior(x), 2, dim=-1)\n",
    "        stats = {'mean':posterior_mean, 'std':posterior_std}\n",
    "        posterior_stoch_state, std = self.get_stoch_state(stats)\n",
    "        posterior_rssm_state = RSSMContState(posterior_mean, std, posterior_stoch_state, deter_state)\n",
    "    return prior_rssm_state, posterior_rssm_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch  \n",
    "def rollout_observation(self: RSSM, seq_len:int, obs_embed: torch.Tensor, action: torch.Tensor, nonterms: torch.Tensor, prev_rssm_state):\n",
    "    priors = []\n",
    "    posteriors = []\n",
    "    for t in range(seq_len):\n",
    "        prev_action = action[t]*nonterms[t]\n",
    "        prior_rssm_state, posterior_rssm_state = self.rssm_observe(obs_embed[t], prev_action, nonterms[t], prev_rssm_state)\n",
    "        priors.append(prior_rssm_state)\n",
    "        posteriors.append(posterior_rssm_state)\n",
    "        prev_rssm_state = posterior_rssm_state\n",
    "    prior = self.rssm_stack_states(priors, dim=0)\n",
    "    post = self.rssm_stack_states(posteriors, dim=0)\n",
    "    return prior, post\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14043d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45511982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa9ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
