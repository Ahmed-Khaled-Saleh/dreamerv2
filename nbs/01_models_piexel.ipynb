{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4bdaba",
   "metadata": {},
   "source": [
    "# Pixels model\n",
    "\n",
    "> Observation model for encoding and decoding pixel images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as td\n",
    "import torch.nn as nn\n",
    "from fastcore.utils import *\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9211e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def conv_out(h_in, padding, kernel_size, stride):\n",
    "    return int((h_in + 2. * padding - (kernel_size - 1.) - 1.) / stride + 1.)\n",
    "\n",
    "def output_padding(h_in, conv_out, padding, kernel_size, stride):\n",
    "    return h_in - (conv_out - 1) * stride + 2 * padding - (kernel_size - 1) - 1\n",
    "\n",
    "def conv_out_shape(h_in, padding, kernel_size, stride):\n",
    "    return tuple(conv_out(x, padding, kernel_size, stride) for x in h_in)\n",
    "\n",
    "def output_padding_shape(h_in, conv_out, padding, kernel_size, stride):\n",
    "    return tuple(output_padding(h_in[i], conv_out[i], padding, kernel_size, stride) for i in range(len(h_in)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ObsEncoder(nn.Module):\n",
    "    def __init__(self, input_shape, embedding_size, info):\n",
    "        \"\"\"\n",
    "        :param input_shape: tuple containing shape of input\n",
    "        :param embedding_size: Supposed length of encoded vector\n",
    "        \"\"\"\n",
    "        super(ObsEncoder, self).__init__()\n",
    "        self.shape = input_shape\n",
    "        activation = info['activation']\n",
    "        d = info['depth']\n",
    "        k  = info['kernel']\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], d, k),\n",
    "            activation(),\n",
    "            nn.Conv2d(d, 2*d, k),\n",
    "            activation(),\n",
    "            nn.Conv2d(2*d, 4*d, k),\n",
    "            activation(),\n",
    "        )\n",
    "        if embedding_size == self.embed_size:\n",
    "            self.fc_1 = nn.Identity()\n",
    "        else:\n",
    "            self.fc_1 = nn.Linear(self.embed_size, embedding_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def forward(self: ObsEncoder, obs):\n",
    "    batch_shape = obs.shape[:-3]\n",
    "    batch_shape = obs.shape[:-3]\n",
    "    img_shape = obs.shape[-3:]\n",
    "    embed = self.convolutions(obs.reshape(-1, *img_shape))\n",
    "    embed = torch.reshape(embed, (*batch_shape, -1))\n",
    "    embed = self.fc_1(embed)\n",
    "    return embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93123d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch(as_prop=True)\n",
    "def embed_size(self: ObsEncoder):\n",
    "    conv1_shape = conv_out_shape(self.shape[1:], 0, self.k, 1)\n",
    "    conv2_shape = conv_out_shape(conv1_shape, 0, self.k, 1)\n",
    "    conv3_shape = conv_out_shape(conv2_shape, 0, self.k, 1)\n",
    "    embed_size = int(4*self.d*np.prod(conv3_shape).item())\n",
    "    return embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39af39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ObsDecoder(nn.Module):\n",
    "    def __init__(self, output_shape, embed_size, info):\n",
    "        \"\"\"\n",
    "        :param output_shape: tuple containing shape of output obs\n",
    "        :param embed_size: the size of input vector, for dreamerv2 : modelstate \n",
    "        \"\"\"\n",
    "        super(ObsDecoder, self).__init__()\n",
    "        c, h, w = output_shape\n",
    "        activation = info['activation']\n",
    "        d = info['depth']\n",
    "        k  = info['kernel']\n",
    "        conv1_shape = conv_out_shape(output_shape[1:], 0, k, 1)\n",
    "        conv2_shape = conv_out_shape(conv1_shape, 0, k, 1)\n",
    "        conv3_shape = conv_out_shape(conv2_shape, 0, k, 1)\n",
    "        self.conv_shape = (4*d, *conv3_shape)\n",
    "        self.output_shape = output_shape\n",
    "        if embed_size == np.prod(self.conv_shape).item():\n",
    "            self.linear = nn.Identity()\n",
    "        else:\n",
    "            self.linear = nn.Linear(embed_size, np.prod(self.conv_shape).item())\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4*d, 2*d, k, 1),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(2*d, d, k, 1),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(d, c, k, 1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef94426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def forward(self: ObsDecoder, x):\n",
    "    batch_shape = x.shape[:-1]\n",
    "    embed_size = x.shape[-1]\n",
    "    squeezed_size = np.prod(batch_shape).item()\n",
    "    x = x.reshape(squeezed_size, embed_size)\n",
    "    x = self.linear(x)\n",
    "    x = torch.reshape(x, (squeezed_size, *self.conv_shape))\n",
    "    x = self.decoder(x)\n",
    "    mean = torch.reshape(x, (*batch_shape, *self.output_shape))\n",
    "    obs_dist = td.Independent(td.Normal(mean, 1), len(self.output_shape))\n",
    "    return obs_dist\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45511982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa9ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
